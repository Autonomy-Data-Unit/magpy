{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2b7e1fb5",
            "metadata": {},
            "source": [
                "# config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "217d2900",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp core.config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "89fbd3f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nbdev.showdoc import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b7a04e29",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "from pathlib import Path\n",
                "from typing import Optional, Union\n",
                "import adulib.llm\n",
                "from adulib.caching import set_default_cache_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "313d9142",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "_current_config = {}\n",
                "\n",
                "def set_magpy_config(\n",
                "    api_key: Optional[str] = None,\n",
                "    model_name: str = \"gpt-4o\",\n",
                "    temperature: float = 0.1,\n",
                "    cache_path: Union[str, Path, None] = None,\n",
                "    caching: bool = True,\n",
                "    **kwargs\n",
                "):\n",
                "    \"\"\"Configure LLM settings for magpy.\n",
                "    \n",
                "    Args:\n",
                "        api_key: API key for the LLM provider\n",
                "        model_name: Name of the model to use (default: gpt-4o)\n",
                "        temperature: Temperature for generation (default: 0.1)\n",
                "        cache_path: Path to cache directory (default: None for default cache)\n",
                "        caching: Whether to enable caching (default: True)\n",
                "        **kwargs: Additional arguments passed to adulib.llm functions\n",
                "    \"\"\"\n",
                "    global _current_config\n",
                "    \n",
                "    config = {\n",
                "        'model': model_name,\n",
                "        'temperature': temperature,\n",
                "        'cache_enabled': caching,\n",
                "        **kwargs\n",
                "    }\n",
                "    \n",
                "    if api_key:\n",
                "        config['api_key'] = api_key\n",
                "    \n",
                "    if caching:\n",
                "        if cache_path:\n",
                "            set_default_cache_path(Path(cache_path))\n",
                "        else:\n",
                "            raise ValueError(\"Cache path must be provided if caching is enabled.\")\n",
                "    \n",
                "    _current_config = config\n",
                "\n",
                "def get_llm_config():\n",
                "    \"\"\"Get current LLM configuration.\"\"\"\n",
                "    return _current_config.copy()\n",
                "\n",
                "def get_model_name():\n",
                "    \"\"\"Get the currently configured model name.\"\"\"\n",
                "    return _current_config.get('model', 'gpt-4o')\n",
                "\n",
                "def get_temperature():\n",
                "    \"\"\"Get the currently configured temperature.\"\"\"\n",
                "    return _current_config.get('temperature', 0.1)\n",
                "\n",
                "def is_caching_enabled():\n",
                "    \"\"\"Check if caching is enabled.\"\"\"\n",
                "    return _current_config.get('cache_enabled', True)"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "cell_metadata_filter": "-all",
            "main_language": "python",
            "notebook_metadata_filter": "-all"
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "45fc2c4efb662dbfa9dd4348808d0728ef45a65eda0e065d93c07024e0671b59"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}