{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b3c868a3",
            "metadata": {},
            "source": [
                "# extractor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ac01375",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp extract.extractor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bf1a9b8a",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nbdev.showdoc import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14e775da",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "from typing import Any, Dict, Union, Type, Optional, List\n",
                "from pathlib import Path\n",
                "import json\n",
                "import pymupdf\n",
                "import io\n",
                "from datetime import datetime\n",
                "import inspect\n",
                "\n",
                "import adulib.llm\n",
                "from pydantic import BaseModel, Field as PydanticField, create_model\n",
                "from magpy.core.config import get_llm_config\n",
                "from magpy.extract.schema import Field, _create_dynamic_model, _pydantic_to_response_format_param"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9650c34",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def extract_structured(\n",
                "    text: Optional[str] = None,\n",
                "    texts: Optional[List[str]] = None,\n",
                "    path: Optional[Union[str, Path]] = None,\n",
                "    paths: Optional[List[Union[str, Path]]] = None,\n",
                "    schema: Dict[str, Union[Type, Field]]|Type[BaseModel] = None,\n",
                "    temperature: float = None,\n",
                "    api_key: str|None = None,\n",
                ") -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n",
                "    \"\"\"Extract structured data from unstructured text using a target schema.\n",
                "    \n",
                "    Args:\n",
                "        text: Single text string to extract from\n",
                "        texts: List of text strings to extract from\n",
                "        path: Single file path to extract from\n",
                "        paths: List of file paths to extract from. The supported file formats are `.pdf`, `.txt`, and `.md`.\n",
                "        schema: Target schema defining fields to extract\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary or list of dictionaries with extracted structured data\n",
                "    \"\"\"\n",
                "    if not schema:\n",
                "        raise ValueError(\"Schema must be provided\")\n",
                "    \n",
                "    # Determine input type and normalize to list of texts\n",
                "    input_texts = []\n",
                "    \n",
                "    if text:\n",
                "        input_texts = [text]\n",
                "    elif texts:\n",
                "        if not type(texts) in [list, tuple]: raise ValueError(\"`texts` argument must be a list or tuple\")\n",
                "        input_texts = texts\n",
                "    elif path:\n",
                "        input_texts = [_load_text_from_file(Path(path))]\n",
                "    elif paths:\n",
                "        if not type(paths) in [list, tuple]: raise ValueError(\"`paths` argument must be a list or tuple\")\n",
                "        input_texts = [_load_text_from_file(Path(p)) for p in paths]\n",
                "    else:\n",
                "        raise ValueError(\"Must provide one of: text, texts, path, or paths\")\n",
                "    \n",
                "    # Create Pydantic model from schema\n",
                "    pydantic_model = _create_dynamic_model(schema) if not issubclass(schema, BaseModel) else schema\n",
                "    api_schema = _pydantic_to_response_format_param(pydantic_model)\n",
                "    \n",
                "    config = get_llm_config()\n",
                "    \n",
                "    results = []\n",
                "    \n",
                "    for input_text in input_texts:\n",
                "        # Create extraction prompt\n",
                "        prompt = _create_extraction_prompt(input_text)\n",
                "        \n",
                "        # Call LLM with structured response format\n",
                "        # Disable caching when using response_format since Pydantic models can't be pickled\n",
                "        response = adulib.llm.single(\n",
                "            prompt=prompt,\n",
                "            model=config.get('model', 'gpt-4o'),\n",
                "            temperature=config.get('temperature', 0.1),\n",
                "            response_format=api_schema,\n",
                "            api_key=config.get('api_key', None),\n",
                "            **{k: v for k, v in config.items() \n",
                "               if k not in ['model', 'temperature', 'cache_enabled', 'api_key']}\n",
                "        )\n",
                "        \n",
                "        # Parse structured response\n",
                "        try:\n",
                "            result = pydantic_model.model_validate_json(response)\n",
                "            results.append(result.model_dump())\n",
                "        except Exception as e:\n",
                "            raise ValueError(f\"Failed to parse LLM response: {e}\\nResponse: {response}\")\n",
                "    \n",
                "    # Return single result or list based on input\n",
                "    if len(results) == 1 and (text or path):\n",
                "        return results[0]\n",
                "    else:\n",
                "        return results\n",
                "\n",
                "def _load_text_from_file(file_path: Path) -> str:\n",
                "    \"\"\"Load text content from various file formats.\"\"\"\n",
                "    if not file_path.exists():\n",
                "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
                "    \n",
                "    suffix = file_path.suffix.lower()\n",
                "    \n",
                "    if suffix == '.pdf':\n",
                "        return _extract_text_from_pdf(file_path)\n",
                "    elif suffix in ['.txt', '.md']:\n",
                "        return file_path.read_text(encoding='utf-8')\n",
                "    else:\n",
                "        # Try to read as text\n",
                "        try:\n",
                "            return file_path.read_text(encoding='utf-8')\n",
                "        except UnicodeDecodeError:\n",
                "            raise ValueError(f\"Unsupported file format: {suffix}\")\n",
                "\n",
                "def _extract_text_from_pdf(file_path: Path) -> str:\n",
                "    \"\"\"Extract text from PDF file.\"\"\"\n",
                "    pages = []\n",
                "    pdf_document = pymupdf.open(file_path)   \n",
                "    for page_num in range(len(pdf_document)):\n",
                "        page = pdf_document.load_page(page_num)\n",
                "        page_text = page.get_text().strip()\n",
                "        pages.append(f\"Page {page_num+1}:\\n{page_text}\")\n",
                "    return \"\\n\\n\\n\".join(pages)\n",
                "\n",
                "def _create_extraction_prompt(text: str) -> str:\n",
                "    \"\"\"Create prompt for structured data extraction.\"\"\"\n",
                "    \n",
                "    prompt = inspect.cleandoc(f\"\"\"Extract structured data from the following text. Follow these guidelines:\n",
                "\n",
                "    1. Extract information that matches the requested fields\n",
                "    2. For optional fields, include them only if the information is clearly present in the text\n",
                "    3. For datetime fields, use ISO format (YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS)\n",
                "    4. For numerical fields, extract only the numeric value without currency symbols or other formatting\n",
                "    5. If a required field cannot be found, use null\n",
                "    6. Be precise and only extract information that is explicitly stated in the text\n",
                "\n",
                "    TEXT TO ANALYZE:\n",
                "    {text}\"\"\")\n",
                "\n",
                "    return prompt"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "cell_metadata_filter": "-all",
            "main_language": "python",
            "notebook_metadata_filter": "-all"
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "457a02e172f3c69a0610b021219b799142acd356264dbd82bbfe9b213923f7e2"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}